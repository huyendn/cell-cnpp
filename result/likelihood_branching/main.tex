\documentclass[11pt]{article}


\usepackage[hmargin={0.6in, 0.6in}, vmargin={0.9in, 0.9in}]{geometry}
\usepackage{amsmath, amsthm, booktabs}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{url} 
\usepackage{tikz}
\usetikzlibrary{automata, arrows}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\LL}{\mathbb{L}}


%\usepackage{fancyheadings}
%\usepackage{hyperref}

\renewcommand{\baselinestretch}{1}

\newtheorem{proposition}{Proposition}

\title{Likelihood of the model for the branching process}
\author{Author}
\date{October 2025}

\begin{document}

\maketitle

Supposed the process starts at time $T_0 = 0$ and the number of starting stem cell $S_0$. The interarrival time of the next event is exponentially distributed

$$\Delta_{T_i} = T_i - T_{i-1} \sim \text{Exp}(r\cdot S_{i-1}), \, i = 1, \cdots, n,$$

where $r$ is the division rate. At the event time $T_i$, the triplet of random variable $(X_i, Y_i, Z_i)$ has the following distribution
\begin{equation}
    (X_i, Y_i, Z_i) = \begin{cases}
        (+1, 0, 0), & p_1(T_i)\\
        (0, +1, 0), & p_2(T_i)\\
        (-1, +2, 0), &p_3(T_i) \\
        (0, 0, +1), & p_4(T_i).
    \end{cases}
\end{equation}
Assume that there is no dud stem cells (i.e $p_4(t) = 0 \, \forall t)$, and we observe all the events (both the time of the events $T_0, T_1, \cdots, T_n$, and the number of stem cells $S_0, S_1, \cdots, S_n$. Since we observe every events, we can know how the cell changes $(X_i, Y_i)$ at each event time.

The likelihood of observing the event times and division types \begin{equation}
    \LL(T_1, T_2, \cdots, T_n, S_1, \cdots, S_i) = \prod_{i=1}^n \Big[p_1(T_i) I_{(X_i=1)} + p_2(T_i) I_{(X_i=0)} + p_3(T_i) I_{(X_i=-1)}\Big] \cdot rS_{i-1}e^{-rS_{i-1} \Delta_{T_i}}.
\end{equation}
Given the division probability
\begin{equation}
\begin{split}
    P(X_i = 1 \vert T_i) & = \frac{p_1}{1 + c(T_i-m)^2} \\
    P(X_i = 0 \vert T_i) & = \frac{p_2}{1 + c(T_i-m)^2} \\
    P(X_i = -1 \vert T_i) & = 1-\frac{p_1 + p_2}{1 + c(T_i-m)^2},
\end{split}
\end{equation}
with $p_1, p_2, c, m > 0, p_1 + p_2 < 1$, the likelihood of observing the event is
\begin{equation}
\begin{split}
       &\LL(T_1, T_2, \cdots, T_n, S_1, \cdots, S_i) \\
       & = \prod_{i=1}^n \Big[ \frac{p_1}{1 + c(T_i-m)^2}I_{(X_i=1)} + \frac{p_2}{1 + c(T_i-m)^2} I_{(X_i=0)} + \Big( 1- \frac{p_1 + p_2}{1 + c(T_i-m)^2}\Big) I_{(X_i=-1)}\Big] rS_{i-1}e^{-rS_{i-1} \Delta_{T_i}}. 
\end{split}
\end{equation}
Let $f(t, c,m) = \frac{1}{1+c(t-m)^2}$, the log-likelihood is
\begin{equation}
    \begin{split}
        &\ell(T_1, T_2, \cdots, T_n, S_1, \cdots, S_n) \\
        &= \sum_{i=1}^n \Big[\log (p_1 \cdot  f(T_i, c,m)) \cdot I_{(X_i = 1)} +  \log (p_2 \cdot  f(T_i, c,m)) \cdot I_{(X_i = 0)} + \log ([ 1- (p_1+p_2)] \cdot  f(T_i, c,m)) \cdot I_{(X_i = -1)}\Big] \\
        & + n \log(r) + \sum_{i=1}^n \log (S_{i-1}) -r\sum_{i = 1}^n S_{i-1} \Delta_{T_i}.
    \end{split}
\end{equation}
We take derivative of the log-likelihood with respect to each parameter $r, p_1, p_2, c, m$
\begin{equation}
    \begin{split}
    \frac{\partial \ell}{\partial r} &= \frac{n}{r} - \sum_{i=1}^n S_{i-1} \Delta_{T_i},\\
    \frac{\partial \ell}{\partial p_1} &= \sum_{i=1}^n \frac{I_{(X_i = 1)}}{p_1} -\sum_{i=1}^n \frac{I_{(X_i = -1)} f(T_i, c,m)}{1-(p_1 + p_2) f(T_i, c, m)}, \\
    \frac{\partial \ell}{\partial p_2} &= \sum_{i=1}^n \frac{I_{(X_i = 0)}}{p_2} -\sum_{i=1}^n \frac{I_{(X_i = -1)} f(T_i, c,m)}{1-(p_1 + p_2) f(T_i, c, m)}, \\
    \frac{\partial \ell}{\partial c} & = \sum_{i=1}^n \frac{I_{(X_i = 1)} + I_{(X_i = 0)}}{f(T_i, c, m)} [-(T_i -m)^2 f(T_i, c,m)] \\
    & - \sum_{i=1}^n \frac{I_{(X_i = -1)} (p_1 + p_2)}{1-(p_1 + p_2 ) f(T_i, c, m)} [-(T_i -m)^2 f(T_i, c,m)], \\
    \frac{\partial \ell}{\partial m} & = \sum_{i=1}^n \frac{I_{(X_i = 1)} + I_{(X_i = 0)}}{f(T_i, c, m)} 2c(T_i-m)[f(T_i,c,m)]^2 \\
    &- \sum_{i=1}^n \frac{I_{(X_i = -1)} (p_1 + p_2)}{1-(p_1 + p_2 ) f(T_i, c, m)}2c(T_i-m)[f(T_i,c,m)]^2.
    \end{split}
\end{equation}
Setting $\frac{\partial \ell}{\partial r} = 0,$ we have 
$$\hat{r} = \frac{n}{\sum_{i=1}^n S_{i-1} \Delta_{T_i}}.$$
So we can get a closed-form solution for the MLE of parameter $r$.

\subsection*{MLE Estimates}
I use the log-likelihood function in equation (5) and optimize it using the Nelder-Mead optimization in R with linear inequality constraints (function constrOptim) to estimates the parameters. I also include the gradients from equation (6) in the optimization. I simulate 100 replications using the parameters $S_0 = 200, r = 0.2, p_1 = 0.5, p_2 = 0.2, c = 0.005, m = 4$. Figure (\ref{fig:prob_function}) shows the probability function given these parameters.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{prob_plot_05020005402.png}
    \caption{Functions $p_1(t), p_2(t), p_3(t)$ with parameters $p_1 = 0.5, p_2 = 0.2, c = 0.005, m = 4.$}
    \label{fig:prob_function}
\end{figure}
The estimates are stable at different starting points. Figure (\ref{fig:start01}) and table (\ref{tab:start01}) show estimates and their summary statistics across 100 replications with starting value $(p_1, p_2, c, m,r) = (0.1, 0.1, 5, 5, 1)$. Figure (\ref{fig:start02}) and table (\ref{tab:start02}) show estimates and their summary statistics across 100 replications with starting value $(p_1, p_2, c, m,r) = (0.2, 0.4, 10, 10, 1)$. Both starting values give really good estimates across all parameters.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{start01_05020005402.png}
    \caption{Violin plot for estimate results using starting values $(p_1, p_2, c, m,r) = (0.1, 0.1, 5, 5, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c, m,r) = (0.5, 0.2, 0.005, 4, 0.2)$.}
    \label{fig:start01}
\end{figure}
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ & $p_2$ &$c$  & $m$ & $r$\\
         \hline
         Mean &  0.506 & 0.201 & 0.00495 & 3.550 & 0.199\\
         Median& 0.511 & 0.199 & 0.00485 & 3.940 & 0.198 \\
         2.5 Percentile& 0.461 & 0.177 & 0.00311 & 0.315 & 0.188 \\
         97.5 Percentile& 0.552 & 0.229 & 0.00723 & 5.590 & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimate results using starting values $(p_1, p_2, c, m,r) = (0.1, 0.1, 5, 5, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c, m,r) = (0.5, 0.2, 0.005, 4, 0.2)$.}
    \label{tab:start01}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{start02_05020005402.png}
    \caption{Violin plot for estimate results using starting values $(p_1, p_2, c, m,r) = (0.2, 0.4, 10, 10, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c, m,r) = (0.5, 0.2, 0.005, 4, 0.2)$.}
    \label{fig:start02}
\end{figure}
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ & $p_2$ &$c$  & $m$ & $r$\\
         \hline
         Mean &  0.506 & 0.201 & 0.00495 & 3.530 & 0.199\\
         Median& 0.511 & 0.200 & 0.00482 & 3.960 & 0.198 \\
         2.5 Percentile& 0.462 & 0.177 & 0.00311 & 0.316 & 0.189 \\
         97.5 Percentile& 0.552 & 0.229 & 0.00723 & 5.590 & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimate results using starting values $(p_1, p_2, c, m,r) = (0.1, 0.1, 5, 5, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c, m,r) = (0.5, 0.2, 0.005, 4, 0.2)$.}
    \label{tab:start02}
\end{table}

\subsection*{Simulation to include tracking of each cell time of division}
I'm also currently working on simulating data to track the time stem cells are created and undergo division. The simulation produces two datasets. The first one is the data that tracks cell counts after each division as we have before (figure (\ref{fig:division}). The second one is the data that tracks the parent cell of each cell and when each cell is created and ceased to exist (undergo division) (figure (\ref{fig:time}). Currently I have this tracking for viable stem cells only. I will add the tracking to the non-viable stem cells and differentiated cells as well. The goal is to used the information of the time cell is created to estimate whether it is a viable or non-viable stem cell.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{trackingdivision.png}
    \caption{Simulated data that tracks the cell counts after each division.}
    \label{fig:division}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{trackingtime.png}
    \caption{Simulated data that tracks parent cells, birth time (when cell is created) and death time (when cell undergoes division and ceases to exists) of stem cell.}
    \label{fig:time}
\end{figure}
\newpage


\subsection*{Add $p_4$ to simplest form of probability function with $p_1, p_2, c, m$}
I tried to add $p_4$, probability of getting non-viable stem cells, to the simplest form of the probability function with parameters $p_1, p_2, c, m$
\begin{equation}
\begin{split}
    P((X_i, Z_i) = (1,0) \vert T_i) & = \frac{p_1}{1 + c(T_i-m)^2} \\
    P((X_i, Z_i) = (0,0) \vert T_i) & = \frac{p_2}{1 + c(T_i-m)^2} \\
    P((X_i, Z_i) = (-1,0) \vert T_i) & = 1-p_4 -\frac{p_1 + p_2}{1 + c(T_i-m)^2} \\
    P((X_i, Z_i) = (0,1)\vert T_i) & = p_4,
\end{split}
\end{equation}
When we observe viable and non-viable stem cells separately, the likelihood of observing the event times and division outcomes
\begin{equation}
    \begin{split}
        &\LL(T_1, T_2, \cdots, T_n, S_1, \cdots, S_i) \\= 
        & \prod_{i=1}^n \Big[p_1(T_i) I_{(X_i=1, Z_i = 0)} + p_2(T_i) I_{(X_i=0, Z_i = 0)} + p_3(T_i) I_{(X_i=-1, Z_i = 0)}+ p_4(T_i) I_{(X_i = 0, Z_i = 1)}\Big] rS_{i-1}e^{-rS_{i-1} \Delta_{T_i}}.
    \end{split}
\end{equation}
I simulate 100 replications using the parameters $S_0 = 200, r = 0.2, p_1 = 0.5, p_2 = 0.2, p_4 = 0.05, c = 0.005, m = 4$. Table (\ref{tab:simple_with_p4_start01}) and (\ref{tab:simple_with_p4_start02}) show the estimate results with two different starting values. The estimates are very stable and accurate. 

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.75\linewidth]{prob_plot_05020005402005.png}
%     \caption{Functions $p_1(t), p_2(t), p_3(t), p_4(t)$ with parameters $p_1 = 0.5, p_2 = 0.2, p_4 = 0.05,  c= 0.005, m = 4$}
%     \label{fig:placeholder}
% \end{figure}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.75\linewidth]{start01_05020005402005.png}
%     \caption{start 01}
%     \label{fig:placeholder}
% \end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ & $p_2$ &$p_4$ &$c$  & $m$ & $r$\\
         \hline
         Mean &  0.502 & 0.202 & 0.049 & 0.00519 & 3.870 & 0.199\\
         Median& 0.500 & 0.200 & 0.050 & 0.00530 & 4.060 & 0.199  \\
         2.5 Percentile& 0.450 & 0.174 & 0.037 & 0.00321 & 0.641 & 0.189 \\
         97.5 Percentile& 0.544 & 0.241 & 0.060 & 0.00741 & 5.730 & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimate results for the simplest form of the probability function with non-viable stem cells. The true parameters are $(p_1, p_2, p_4, c, m) = (0.5, 0.2, 0.05, 0.005, 4)$. The starting values are $(p_1, p_2, p_4, c, m) = (0.1, 0.1, 0.1, 5, 5, 1)$.}
    \label{tab:simple_with_p4_start01}
\end{table}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.75\linewidth]{start02_0502000540205.png}
%     \caption{start02}
%     \label{fig:placeholder}
% \end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ & $p_2$ &$p_4$ &$c$  & $m$ & $r$\\
         \hline
         Mean &  0.502 & 0.202 & 0.049 & 0.00519 & 3.880 & 0.199\\
         Median& 0.500 & 0.200 & 0.050 & 0.00529 & 4.080 & 0.199  \\
         2.5 Percentile& 0.450 & 0.174 & 0.037 & 0.00320 & 0.641 & 0.189 \\
         97.5 Percentile& 0.544 & 0.241 & 0.060 & 0.00741 & 5.730 & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimate results for the simplest form of the probability function with non-viable stem cells. The true parameters are $(p_1, p_2, p_4, c, m) = (0.5, 0.2, 0.05, 0.005, 4)$. The starting values are $(p_1, p_2, p_4, c, m) = (0.2, 0.4, 0.1, 20, 20, 1)$.}
    \label{tab:simple_with_p4_start02}
\end{table}
\newpage
\clearpage
\subsection*{Variance of Cell Count in Branching Process}
\subsubsection*{Derivation of the theoretical expectation and variance}
Let $\Delta > 0$. 
\begin{equation}
    \begin{split}
        E[X(t + \Delta) - X(t) \vert X(t)] & = [p_1(t) - p_3(t) + \mathcal{O}(\Delta)] \cdot r X(t) \Delta + \mathcal{o}(\Delta) \\
        & = [p_1(t) - p_3(t)] rX(t) + \mathcal{o}(\Delta).
    \end{split}
\end{equation}
Taking expectation,
\begin{equation}
    \begin{split}
        S(t + \Delta) - S(t) = [p_1(t) - p_3(t)] r S(t) \Delta + E[\xi] + \mathcal{o}(\Delta).
    \end{split}
\end{equation}
Let $\Delta \rightarrow 0$,
\begin{equation}
    \frac{dS(t)}{dt} = [p_1(t) - p_3(t)]rS(t).
\end{equation}
Thus, the expected stem cell count of the branching process coincides with the differential equation.

Let $V(t)$ denote the theoretical variance of the stem cell in the branching process. Denote $S(t) = E[X(t)], M(t) = E[X(t)^2]$, then $V(t) = M(t) - S(t)^2$. Let $\Delta > 0$.
\begin{equation}
    \begin{split}
        E[X(t + \Delta)^2 - X(t)^2 \vert X(t)] & = [2 X(t) (p_1(t) - p_3(t)) + (p_1(t) + p_3(t))] rX(t) \Delta\\
        & = 2 X(t)^2(p_1(t)-p_3(t)) r \Delta + X(t) (p_1(t) + p_3(t) ) r\Delta.
    \end{split}
\end{equation}
Taking expectation,
\begin{equation}
    \begin{split}
        M(t + \Delta) - M(t) = 2[p_1(t) -p_3(t)] rM(t) \Delta + [p_1(t) + p_3(t)] rS(t) \Delta.
    \end{split}
\end{equation}
Let $\Delta \rightarrow0$,
\begin{equation}
\begin{split}
    M'(t) &= 2[p_1(t) -p_3(t)]rM(t) + [p_1(t) + p_3(t)]rS(t),\\
    V'(t) & = M'(t)  - 2S(t) S'(t) \\
    & = 2[p_1(t) - p_3(t)] rM(t) + [p_1(t) + p_3(t)]rS(t) - 2S(t)[p_1(t) - p_3(t)]rS(t) \\
    & = 2[p_1(t) -p_3(t)]rV(t) + [p_1(t) + p_3(t)]r S(t).
\end{split}
\end{equation}
\subsubsection*{Verify the theoretical variance with simulation}
Figure (\ref{fig:var_compare}) displays the plots to compare variance from the branching process with variance from the compound nonhomogeneous Poisson process, using the form of the probability function 
\begin{equation}
    \begin{split}
        P(X_i = 1 \vert T_i) &= \frac{p_1}{1 + c_1 (T_i -m_1)^2}, \\
        P(X_i = 0 \vert T_i) & = \frac{p_2}{1 + c_2 (T_i - m_2)^2}, \\
        P(X_i = -1 \vert T_i ) & = 1- \frac{p_1}{1+c_1(T_i -m_1)^2} - \frac{p_2}{1 + c_2(T_i -m_2)^2}.
    \end{split}
\end{equation}
The parameters used to construct these plots are $S_0 = 200, r = 0.2, p_1 = 0.5, c_1 = 0.005, m_1 = 4, p_2 = 0.2, c_2 = 0.1, m_2 = 12$. Figure (\ref{fig:var_compare}) on the left is the plot of variances over time, and on right is the theoretical mean and the region within two standard deviations from the mean. In this set-up, the theoretical variances of the two processes start out similar in the beginning, however, as $t$ gets large, the variance of the branching process converges to 0, whereas the variance of the compound nonhomogeneous Poisson process converges to a positive number. It is also possible for the variance of the branching process to be greater than that of the compound Poisson process. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.45\linewidth]{variance_comparison.png}
    \includegraphics[width=0.45\linewidth]{sdfrommean.png}
    \caption{Comparing theoretical variances of the two process with parameters $S_0 = 200, r = 0.2, p_1 = 0.5, c_1 = 0.005, m_1 = 4, p_2 = 0.2, c_2 = 0.1, m_2 = 12$}
    \label{fig:var_compare}
\end{figure}
Figure (\ref{fig:var_sim}) compares the theoretical variances with the variability from the simulated data. 50 replications of the simulated data with the same parameters as in figure (\ref{fig:var_compare}). The simulated cell count data is plotted, along with the theoretical mean and the region within 2 theoretical standard deviation from the mean. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\linewidth]{bpCI.png}
    \includegraphics[width=0.45\linewidth]{cpnnCI.png}
    \caption{Compare theoretical variances with the simulated data with parameters $S_0 = 200, r = 0.2, p_1 = 0.5, c_1 = 0.005, m_1 = 4, p_2 = 0.2, c_2 = 0.1, m_2 = 12$.}
    \label{fig:var_sim}
\end{figure}
\subsubsection*{Show variance of the stem cell is finite }
$$V'(t) = 2[p_1(t) - p_3(t)] r V(t) + [p_1(t) + p_3(t)]r S(t),$$
\begin{equation}
    \begin{split}
        \Rightarrow V(t) &= \exp \Big\{ 2r\int_0^t [p_1(u) -p_3(u)] du  \Big\} \Big[\int_0^t [p_1(u) + p_3(u)] r S(u)  \exp \Big\{ -2r\int_0^u [p_1(v) -p_3(v)] dv  \Big\}du + C \Big] \\
        & = \exp \Big\{ 2r\int_0^t [p_1(u) -p_3(u)] du  \Big\} \\
        & \quad
        \Big[\int_0^t [p_1(u) + p_3(u)] r S_0 \exp \Big\{ r\int_0^u [p_1(v) -p_3(v)] dv  \Big\} \exp \Big\{ -2r\int_0^u [p_1(v) -p_3(v)] dv  \Big\}du + C \Big] \\
        & = S_0 \cdot r \cdot \exp \Big\{ 2r\int_0^t [p_1(u) -p_3(u)] du  \Big\} \Big[\int_0^t [p_1(u) + p_3(u)] 
        \exp \Big\{ -r\int_0^u [p_1(v) -p_3(v)] dv  \Big\}du + C \Big].
    \end{split}
\end{equation}
Using the initial condition $V(0) = 0$, we can simplify the expression of $V(t)$ in terms of $S(t)$

$$V(t) = r S(t)^2 \int_0^t \frac{p_1(u) + p_3(u)}{S(u)} du.$$

Let $P(t) = \int_0^t [p_1(u) -p_3(u)] du$. Since $p_1(u) + p_3(u) \leq 1 \ \forall u$ and $r$ is a positive constant, to show $V(t) < \infty$ as $t \rightarrow \infty$, we can show 
$$f (t) = \exp \Big\{ P(t) \Big\} \Big[\int_0^t\exp \Big\{ - P(u)  \Big\}du \Big] < \infty $$ as $t \rightarrow \infty$. 
When $P(t) \rightarrow \infty$, $\exp \Big\{  P(t) \Big\} \rightarrow \infty$ and $\int_0^t\exp \Big\{ - P(u)  \Big\}du < \infty$ as $t \rightarrow \infty$. Then $f(t) \rightarrow \infty$ as $t\rightarrow \infty$. Thus, we consider when $P(t) \rightarrow - \infty$ as $t \rightarrow \infty$.

We rewrite $f(t)$ as a quotient
$$f(t) = \frac{\int_0^t\exp \Big\{ - P(u)  \Big\}du }{\exp \Big\{ - P(t) \Big\}}.$$
Both numerator and denominator go to $+ \infty$ as $t \rightarrow \infty$, we can use the l'H\^opital's rule.
$$\lim_{t\rightarrow \infty} f(t) = \lim_{t\rightarrow \infty} \frac{\int_0^t\exp \Big\{ - P(u)  \Big\}du}{\exp \Big\{ - P(t) \Big\}} = \lim_{t\rightarrow \infty} = \frac{\exp \{ - P(t) \}} {-[p_1(t) - p_3(t)] \exp \{ - P(t) \}} =  \lim_{t\rightarrow \infty} \frac{-1}{p_1(t) - p_3(t)}.$$

\textbf{Case 1.} $\lim_{t\rightarrow \infty} [p_1(t) - p_3(t)] \rightarrow - p$ for some $p > 0$. Then $\lim_{t\rightarrow \infty} f(t)$ is a finite constant.

\textbf{Case 2.} $\lim_{t\rightarrow \infty} [p_1(t) - p_3(t)] \rightarrow - \infty$. Then $\lim_{t\rightarrow \infty} f(t) \rightarrow 0$.

\textbf{Case 3.} $[p_1(t) - p_3(t)]$ does not converge but is negative on average $$\limsup_{T\rightarrow\infty} {\frac{1}{T} \int_0^T [p_1(u) - p_3(u)] du = \limsup_{T\rightarrow\infty} \frac{P(T)}{T} = \bar{p}} < 0.$$
Then on average $P(t)$ decreases roughly like $\bar{p}t$ for large $t$ with some remainder term $P(t) = \bar{p}t + R(t)$, that is bounded or growing slower than linear. Then
\begin{equation}
    \begin{split}
        f (t) &= \exp \Big\{ P(t) \Big\} \Big[\int_0^t\exp \Big\{ - P(u)  \Big\}du\Big] \\
        & = \exp \{\bar{p}t + R(t)\} \int_0^t \exp \{-\bar{p}u - R(u)\} du \\
        & = \exp \{\bar{p}t\} \exp\{R(t)\} \int_0^t \exp \{\vert\bar{p} \vert u\} \exp \{-R(u)\} du \ (\text{since } \bar{p} < 0). 
    \end{split}
\end{equation}
Since $R(t)$ oscillates and is bounded, $\exp\{-R(u)\}$ is bounded. Thus, $\int_0^t \exp \{\vert\bar{p} \vert u\} \exp \{-R(u)\} du  \approx C_1 \exp \{\vert\bar{p} \vert t\} $ for some $C > 0$. Then
$$f(t) \approx \exp \{\bar{p}t\} \exp\{R(t)\} C_1 \exp \{\vert\bar{p} \vert t\} = C_1 \exp\{R(t)\}.$$
Since $\exp\{R(u)\}$ is bounded, $f(t)$ is bounded. Thus, as $t \rightarrow \infty$ the variance $V(t)$ is finite when $\int_0^t [p_1(u) - p_3(u)]du \rightarrow -\infty$. $V(t) \rightarrow 0$ when $[p_1(t) - p_3(t)] \rightarrow -\infty$. 
\newpage

\subsection*{Add parameters for $p_2(t)$}
Suppose we have the division probabilities
\begin{equation}
    \begin{split}
        P(X_i = 1 \vert T_i) & = \frac{p_1}{1+c_1(T_i-m_1)^2},\\
        P(X_i = 0 \vert T_i) & = \frac{p_2}{1+c_2(T_i-m_2)^2},\\
        P(X_i = -1 \vert T_i) & = 1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2},
    \end{split}
\end{equation}
with $p_1, p_2, c, m > 0, p_1 + p2 < 1$. The likelihood of observing the event is 
\begin{equation}
    \begin{split}
        \LL (T_1, \cdots, T_n, S_0, \cdots, S_n) 
        = \prod_{i=1}^n \Big[& \frac{p_1}{1+c_1(T_i-m_1)^2} I_{(X_i = 1)} + \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(X_i = 0)} \\
        &+ \Big( 1- \frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(X_i = -1)} \Big) \Big] rS_{i-1} e^{-rS_{i-1} \Delta_{T_i}}.
    \end{split}
\end{equation}
The log-likelihood is
\begin{equation}
    \begin{split}
        \ell(T_1, \cdots, T_n, S_0, \cdots, S_n) = \sum_{i=1}^n \Big[ &\log \Big(\frac{p_1}{1+c_1(T_i-m_1)^2}\Big) I_{(X_i = 1)} \\
        & + \log \Big(\frac{p_2}{1+c_2(T_i-m_2)^2}\Big) I_{(X_i = 0)} \\
        & + \log \Big(1- \frac{p_1}{1+c_1(T_i-m_1)^2}- \frac{p_2}{1+c_2(T_i-m_2)^2} \Big) I_{(X_i = -1)} \\
        & + \log r + \log S_{i-1} - rS_{i-1}\Delta_{T_1} \Big].
    \end{split}
\end{equation}
We take the derivative of the log-likelihood with respect to each parameter $r, p_1, p_2, c_1, c_2, m_1, m_2$.
\begin{equation}
    \begin{split}
        \frac{\partial \ell}{\partial r} & = \frac{n}{r} - \sum_{i=1}^nS_{i-1} \Delta_{T_I} \\
        \frac{\partial \ell}{\partial p_1} & = \sum_{i=1}^n \frac{I_{(X_i = 1)}}{p_1} - \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}}\frac{1}{1+c_1(T_i-m_1)^2}\\
        \frac{\partial \ell}{\partial p_2} & = \sum_{i=1}^n \frac{I_{(X_i = 0)}}{p_2} - \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}}\frac{1}{1+c_1(T_i-m_1)^2}\\
        \frac{\partial \ell}{\partial c_1} & = - \sum_{i=1}^n \frac{I_{(X_i = 1)} (T_i -m_1)^2}{1+c_1(T_i-m_1)^2}- \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}} \Big( - \frac{p_1(T_i-m_1)^2}{[1+c_1(T_i-m_1)^2]^2}\Big)\\
        \frac{\partial \ell}{\partial c_2} & = - \sum_{i=1}^n \frac{I_{(X_i = 0)} (T_i -m_2)^2}{1+c_2(T_i-m_2)^2} - \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}}\Big( - \frac{p_2(T_i-m_2)^2}{[1+c_2(T_i-m_2)^2]^2}\Big) \\
        \frac{\partial \ell}{\partial m_1} & = \sum_{i=1}^n \frac{I_{(X_i = 1)} 2c_1 (T_i -m_1)}{1+c_1(T_i-m_1)^2}- \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}} \frac{2p_1 c_1(T_i-m_1)}{[1+ c_1(T_1-m_1)^2]^2}\\
        \frac{\partial \ell}{\partial m_2} & = \sum_{i=1}^n \frac{I_{(X_i = 0)} 2c_2(T_i -m_2)}{1+c_2(T_i-m_2)^2}- \sum_{i=1}^n \frac{I_{(X_i = -1)}}{1-\frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}}\frac{2p_2 c_2(T_i-m_2)}{[1+ c_2(T_i-m_2)^2]^2}
    \end{split}
\end{equation}

% I simulate 100 replications using the parameters $S_0 = 200, r = 0.2, p_1 = 0.5, p_2 = 0.2, c_1 = 0.005, c_2 = 0.01, m_1 = 4, m_2 = 12$. Figure (\ref{fig:diffc_diffm}) shows the graphs of the probability function with these parameters. 

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.75\linewidth]{prob_plot_diffc_diffm.png}
%     \caption{Function $p_1(t) = \frac{p_1}{1+ c_1(t-m_1)^2},p_2(t) = \frac{p_2}{1+ c_2(t-m_2)^2}, p_3(t) = 1- p_1(t)-p_2(t)$ with parameters $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.5, 0.2, 0.005, 0.01, 4, 12)$}
%     \label{fig:diffc_diffm}
% \end{figure}

% I use the similar R function to optimize the log-likelihood function as before. The estimates were not good so I used the closed form solution of $\hat{r}$ to estimate the division rate $r$ and used the \textit{constrOptim} to optimize the condition likelihood given the time sequences $T_i's$ to obtain the parameters $p_1, p_2, c_1, c_2, m_1, m_2$. The estimates of $p_1$ and $p_2$ are pretty stable and pretty close to the true parameters. However, the estimates of other parameters, especially $m_1$ and $m_2$, are dependent of the starting values. Table (\ref{tab:diffc_diffm_start01}) and (\ref{tab:diffc_diffm_start02}) with two different starting values. The starting values used to obtained the estimates in (\ref{tab:diffc_diffm_start02}) have closer $m_1$ and $m_2$ to their true parameter values. 

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|c|}
%     \hline
%          Parameter& $p_1$ & $p_2$ &$c_1$ &$c_2$ & $m_1$ & $m_2$ & $r^*$\\
%          \hline
%          Mean &  0.494 & 0.166 & 0.00438 & 0.00120 & 2.490 & 1.222 & 0.199\\
%          Median& 0.495 & 0.164 & 0.00365 & 0.00110 & 1.500 & 1.080 & 0.200  \\
%          2.5 Percentile& 0.442 & 0.140 & 0.00219 & 0.000473 & 0.0134 & 0.00110 & 0.189 \\
%          97.5 Percentile& 0.555 & 0.195 & 0.00706 & 0.00266 & 7.370 & 3.310 & 0.209\\
%          \hline
%     \end{tabular}
%     \caption{Parameters estimate results using starting values $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.1, 0.1, 0.1, 0.1, 1, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.5, 0.2, 0.005, 0.01, 4, 12)$. *Estimates of $r$ is obtained from the closed form solution. }
%     \label{tab:diffc_diffm_start01}
% \end{table}



% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|c|}
%     \hline
%          Parameter& $p_1$ & $p_2$ &$c_1$ &$c_2$ & $m_1$ & $m_2$ & $r^*$\\
%          \hline
%          Mean &  0.501 & 0.187 & 0.00346 & 0.00698 & 1.38 & 9.440 & 0.199\\
%          Median& 0.500 & 0.186 & 0.00339 & 0.00583 & 1.250 & 9.470 & 0.200  \\
%          2.5 Percentile& 0.453 & 0.149 & 0.00210 & 0.00127 & 0.00149 & 4.610 & 0.189 \\
%          97.5 Percentile& 0.563 & 0.241 & 0.00532 & 0.0193 & 3.910  & 14.200 & 0.209\\
%          \hline
%     \end{tabular}
%     \caption{Parameters estimate results using starting values $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.2, 0.7, 0.2, 0.2, 3, 10)$ with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.5, 0.2, 0.005, 0.01, 4, 12)$. *Estimates of $r$ is obtained from the closed-form solution. }
%     \label{tab:diffc_diffm_start02}
% \end{table}

% Instead of having different $c_1$ and $c_2$, I also tried having just one scale parameter $c$. The estimates are shown in table (\ref{tab:samec_diffm_start01}) (\ref{tab:samec_diffm_start02}). Similarly to when we have different $c_1$ and $c_2$, the estimates of $p_1$ and $p_2$ are stable, but estimates of $m_1$ and $m_2$ are not good.

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%          Parameter& $p_1$ & $p_2$ &$c$ & $m_1$ & $m_2$ & $r^*$\\
%          \hline
%          Mean &  0.450 & 0.223  & 0.00280  & 3.950 & 1.260     & 0.199\\
%          Median&  0.443 & 0.223  & 0.00256 & 3.820 & 0.368 & 0.198  \\
%          2.5 Percentile& 0.386 & 0.199  & 0.00173 & 0.0430 &0.00016 & 0.189 \\
%          97.5 Percentile& 0.498 & 0.251 & 0.00448 & 8.870 & 4.440 & 0.211\\
%          \hline
%     \end{tabular}
%     \caption{Parameters estimate results with same $c$ but different $m_1$ and $m_2$ using starting values $(p_1, p_2, c, m_1,m_2) = (0.1, 0.1, 0.1, 1, 1)$ with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.5, 0.2, 0.005, 4, 12)$. *Estimates of $r$ is obtained from the closed-form solution.}
%     \label{tab:samec_diffm_start01}
% \end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%          Parameter& $p_1$ & $p_2$ &$c$ & $m_1$ & $m_2$ & $r^*$\\
%          \hline
%          Mean &  0.504 & 0.195 & 0.00317 & 0.986 & 8.610 & 0.199\\
%          Median& 0.507 & 0.193 & 0.00319 & 0.374 & 8.930 & 0.198  \\
%          2.5 Percentile& 0.443 & 0.171 & 0.00236 & 0.00485 & 3.960 & 0.189 \\
%          97.5 Percentile& 0.552 & 0.229 & 0.00416 & 4.470 & 13.400 & 0.211\\
%          \hline
%     \end{tabular}
%     \caption{Parameters estimate results with same $c$ but different $m_1$ and $m_2$ using starting values $(p_1, p_2, c, m_1,m_2) = (0.2, 0.7, 0.2, 3, 10)$ with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.5, 0.2, 0.005, 4, 12)$. *Estimates of $r$ is obtained from the closed-form solution.}
%     \label{tab:samec_diffm_start02}
% \end{table}
I reviewed the code for the optimization of the log-likelihood function when we have the probability function as in (12) with different scale and location parameters ($c_1, c_2, m_1, m_2$) for $p_1(t)$ and $p_2(t)$. From the optimization results, the estimates for $p_1$ and $p_2$ are very stable, however, the estimates for $c_1, c_2, m_1, m_2$ are not. I ran the optimization multiple times with different starting points and chose the results with the highest log-likelihood. The starting points are shown in table (\ref{tab:starting_points}). Since $p_1$ and $p_2$ are not sensitive to starting values, the starting values for these two parameters are the same, whereas starting values for $c_1, c_2, m_1, m_2$ describe different scenarios of where the peaks and how sharp the peaks are in functions $p_1(t)$ and $p_2(t)$.
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Scenarios & $p_1$ & $p_2$ &$c_1$ &$c_2$ & $m_1$ & $m_2$ \\
         \hline
         Early peaks&	0.3&	0.3&	0.05&	0.05&	10&	10\\
         Very early peaks&	0.3&	0.3&	0.05&	0.05&	5&	5\\
         Late peaks&	0.3&	0.3&	0.05&	0.05&	45&	45\\
         Sharp peak at center&	0.3&	0.3&	0.2&	0.2&	25&	25\\
         Broad peak at center&	0.3&	0.3&	0.001&	0.001&	25&	25\\
         Asymmetric time centers&	0.3&	0.3&	0.02&	0.03&	35&	15\\
         Asymmetric time centers&	0.3&	0.3&	0.1&	0.05&	20&	30\\
         Random start & unif(0.2,0.5) & unif(0.2,0.5) & unif(0.001,0.05) & unif(0.001,0.05) & unif(0,50) & unif(0,50)\\
         \hline
    \end{tabular}
    \caption{Multiple starting points for optimizations}
    \label{tab:starting_points}
\end{table}

Table (\ref{tab:multiple_start_case1}) and (\ref{tab:multiple_start_case2}) shows the estimate results for simulated data of two sets of parameters using the same multiple starting values. Both results are good and converge to the true parameters. I will try different optimization techniques to see if it would be less sensitive to starting points.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ (0.55)& $p_2$ (0.15) &$c_1$ (0.005) &$c_2$ (0.01) & $m_1$ (4)& $m_2$ (18) \\
         \hline
         Mean & 0.560 & 0.152& 0.00491 & 0.01120	& 3.340 & 17.600 \\
         Median	& 0.560 & 0.152 & 0.00479 & 0.01110 &3.820& 17.600 \\
         2.5 Percentile&	0.503&	0.107&	0.00292& 0.00374 &	0.0000416 & 14.200 \\
         97.5 Percentile &	0.621&	0.192&	0.00759&	0.02150&	6.570&	20.000\\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using multiple starting values with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.55, 0.15, 0.005, 0.01, 4, 18)$.}
    \label{tab:multiple_start_case1}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ (0.2) & $p_2$ (0.7) &$c_1$ (0.005) &$c_2$ (0.01) & $m_1$ (12) & $m_2$ (6) \\
         \hline
         Mean&	0.200&	0.699&	0.00504&	0.01030&	10.700&	6.090\\
         Median&	0.197&	0.697&	0.00454&	0.01020&	10.400&	6.190\\
         2.5 Percentile&	0.163&	0.639&	0.00115&	0.00679&	5.590&	4.540\\
         97.5 Percentile&	0.244&	0.761&	0.01110&	0.01490&	14.800&	7.130\\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using multiple starting values with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.2, 0.6, 0.005,0.1, 12, 6)$.}
    \label{tab:multiple_start_case2}
\end{table}

\newpage

\subsection*{Stopping times}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\linewidth]{stoppingtime_start50_rep100.png}
    \includegraphics[width=0.45\linewidth]{stoppingtime_start50_rep1000.png}
    \includegraphics[width=0.45\linewidth]{stoppingtime_start200_rep100.png}\includegraphics[width=0.45\linewidth]{stoppingtime_start200_rep1000.png}
    \includegraphics[width=0.45\linewidth]{stoppingtime_start1000_rep100.png}
    \includegraphics[width=0.45\linewidth]{stoppingtime_start1000_rep1000.png}
    \caption{Distributions of stopping times from simulated data with varying numbers of initial stem cells and replication counts. The number of starting stem cells is 50 (top row), 200 (middle row), and 1000 (bottom row). Each column represents a different number of replications: 100 (left column) and 1000 (right column). The parameters for the probability function is $p_1 = 0.55, c_1 = 0.005, m_1 =4, p_2 = 0.15, c_2 = 0.01, m_2 = 18$ and division rate is $r = 0.2$. }
    \label{fig:placeholder}
\end{figure}
\newpage
\textbf{Fitting the inverse Gaussian distribution to the shifted stopping times}

We first derive the minimum stopping times. Since the interarrival times between two consecutive division events is the a exponential distribution with rate proportional to the current viable stem cells. Then, the average interarrival time between two consecutive division events $T_{i+1} - T_i$ is $1/(rS_i)$. The minimum stopping times occurs when at each division event is a differentiation event, in which no new viable stem cell is created and the number of viable stem cells decreases by 1. Then the minimum stopping time is
$$\tau_{\min} = \frac{1}{rS_0} + \frac{1}{r (S_0 -1)} + \cdots + \frac{1}{1}.$$
Using the harmonic sum approximation,
$$\tau_{\min} \approx r\log(S_0) + \gamma,$$
where $\gamma \approx 0.5772$ is the Eulerâ€“Mascheroni constant. To fit the inverse Gaussian distribution, we shift the stopping times by $-\tau_{\min}$. 

In the following, we fit the inverse Gaussian distributions to the stopping times of 1000 simulated datasets with the parameters $(p_1, p_2, c_1,c_2, m_1, m_2) = (0.55, 0.15, 0.005, 0.01, 4, 18)$ for the division probabilities, division rate $r = 0.2$, and three different number of initial stem cells $S_0 = 50, 200, 1000$.

For each scenario of initial stem cells, 
\begin{itemize}
    \item The simulated stopping time by $-\tau_{\min}$.
    \item The mean and shape parameters of the inverse Gaussian distribution are estimated using the MLE (using \textit{fitdist} function in the package \textit{fitdistrplus}.
    \item Distributional tests (Kolmogorov-Smirnov and Anderson-Darling) to evaluate the shifted simulated stopping times for the inverse Gaussian distribution.
    \item The shifted simulated data are also fitted for the Gamma and Log-normal distribution. The goodness of fit are compared between the three distributions using the AIC and BIC criteria. 
\end{itemize}

In scenario 1 and 2, with the initial stem cells being 50 and 200, respectively, the distributional tests indicate that the inverse Gaussian distribution is a good fit for the shifted simulated stopping times. Using the AIC and BIC criteria, the fit of the inverse Gaussian distribution and of the log-normal distribution are very compatible, both are good fit for shifted stopping time.

However, in scenario 3 with the initial stem cells being 1000, the distributional tests indicate a poor fit of the inverse Gaussian distribution for the shifted simulated stopping times. Neither the gamma or the log-normal distribution has better fit.
\newpage
\textbf{Scenario 1.} $S_0 = 50$
When $S_0 = 50$, the minimum stopping times is $\tau_{\min} \approx r\log(50) + \gamma =  20.1373$. 


\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         & Estimates & Std. Error \\
         \hline
         Mean  & 29.1554 & 0.2356\\
         Shape & 446.1868 & 19.9544 \\
         \hline
    \end{tabular}
    \caption{MLE for mean and shape parameters for the inverse Gaussian distribution with initial stem cells of 50 and 1000 replications.}
    \label{tab:stoppingtime_start50_rep1000_invgaus_parm}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{stoppingtime_start50_rep1000_invgaus.png}
    \caption{Diagnostic plots for the fit of shifted stopping times (with initial stem cells of 50) for inverse Gaussian distribution.}
    \label{fig:stoppingtime_start50_rep1000_invgaus_diagnostics}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         Test & Test Statistics & P-value \\
         \hline
         Kolmogorov-Smirnov & 0.0189 & 0.8637\\
         Anderson-Darling & 0.5752 & 0.6717\\
         \hline
    \end{tabular}
    \caption{Distributional tests to evaluate the shifted stopping times (with initial stem cells of 50) for inverse Gaussian distribution.}
    \label{tab:stoppingtime_start50_rep1000_invgaus_dist}
\end{table}


\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Inverse Gaussian  & Gamma & Log Normal \\
         \hline
         AIC & 6763.453 &   6778.186  &  6761.191 \\
         BIC & 6773.269 &   6788.001  &  6771.006\\
         \hline
    \end{tabular}
    \caption{Comparing the fit of shifted stopping times (with initial stem cells of 50) with inverse Gaussian, Gamma, and Log-normal distributions.}
    \label{tab:stoppingtime_start50_rep1000_compare_dist}
\end{table}

\newpage
\textbf{Scenario 2.} $S_0 = 200$
When $S_0 = 200$, the minimum stopping times is $\tau_{\min} \approx r\log(200) + \gamma = 27.0688$. 

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         & Estimates & Std. Error \\
         \hline
         Mean  & 30.0482 &  0.2093\\
         Shape & 619.2376 & 27.6928  \\
         \hline
    \end{tabular}
    \caption{MLE for mean and shape parameters for the inverse Gaussian distribution with initial stem cells of 200 and 1000 replications.}
    \label{tab:stoppingtime_start200_rep1000_invgaus_parm}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{stoppingtime_start200_rep1000_invgaus.png}
    \caption{Diagnostic plots for the fit of shifted stopping times (with initial stem cells of 200) for inverse Gaussian distribution.}
    \label{fig:stoppingtime_start200_rep1000_invgaus_diagnostics}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         Test & Test Statistics & P-value \\
         \hline
         Kolmogorov-Smirnov & 0.0233 & 0.6475\\
         Anderson-Darling & 0.8613 & 0.4388\\
         \hline
    \end{tabular}
    \caption{Distributional tests to evaluate the shifted stopping times (with initial stem cells of 200) for inverse Gaussian distribution.}
    \label{tab:stoppingtime_start200_rep1000_invgaus_dist}
\end{table}


\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Inverse Gaussian  & Gamma & Log Normal \\
         \hline
         AIC & 6549.471 &   6568.875  &  6549.636 \\
         BIC & 6559.286 &   6578.691  &  6559.452\\
         \hline
    \end{tabular}
    \caption{Comparing the fit of shifted stopping times (with initial stem cells of 200) with inverse Gaussian, Gamma, and Log-normal distributions.}
    \label{tab:stoppingtime_start200_rep1000_compare_dist}
\end{table}

\newpage
\textbf{Scenario 3.} $S_0 = 1000$

When $S_0 = 1000$, the minimum stopping times is $\tau_{\min} \approx r\log(1000) + \gamma = 35.1159$. 

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         & Estimates & Std. Error \\
         \hline
         Mean  & 31.3807 &  0.2140\\
         Shape & 674.5831 & 30.1662  \\
         \hline
    \end{tabular}
    \caption{MLE for mean and shape parameters for the inverse Gaussian distribution with initial stem cells of 1000 and 1000 replications.}
    \label{tab:stoppingtime_start1000_rep1000_invgaus_parm}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{stoppingtime_start1000_rep1000_invgaus.png}
    \caption{Diagnostic plots for the fit of shifted stopping times (with initial stem cells of 1000) for inverse Gaussian distribution.}
    \label{fig:stoppingtime_start1000_rep1000_invgaus_diagnostics}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         Test & Test Statistics & P-value \\
         \hline
         Kolmogorov-Smirnov & 0.0507 & 0.0114\\
         Anderson-Darling & 4.4769 & 0.0051\\
         \hline
    \end{tabular}
    \caption{Distributional tests to evaluate the shifted stopping times (with initial stem cells of 1000) for inverse Gaussian distribution.}
    \label{tab:stoppingtime_start1000_rep1000_invgaus_dist}
\end{table}


\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Inverse Gaussian  & Gamma & Log Normal \\
         \hline
         AIC & 6596.175 &   6634.945  &  6596.631 \\
         BIC & 6605.991  &  6644.761  &  6606.447\\
         \hline
    \end{tabular}
    \caption{Comparing the fit of shifted stopping times (with initial stem cells of 1000) with inverse Gaussian, Gamma, and Log-normal distributions.}
    \label{tab:stoppingtime_start1000_rep1000_compare_dist}
\end{table}
\clearpage
\newpage

\subsection*{Verify the autocorrelation formula}
For $t > u$,
\begin{equation}
    \begin{split}
        E[X(t) \cdot X(u)] &= E[E[X(t)\cdot X(u) \vert X(u)]] \\
        & = E[X(u) \cdot E[(X(t) \vert X(u)]] \\
        & = E[X(u) \cdot X(u) \cdot \exp \{r [P(t) - P(u)]\}] \\
        & = E[X(u)^2] \cdot \frac{S(t)}{S(u)}.
    \end{split}
\end{equation}
From the variance derivation, we have
$$ E[X(u)^2] = S(u)^2\Big[1 + r\int_0^t \frac{p_1(v) + p_3(v)}{S(v)} dv\Big].$$
Then,
\begin{equation}
    \begin{split}
        E[X(t) \cdot X(u)] - S(t)\cdot S(u) &= S(t) \cdot S(u) \cdot r \int_0^u \frac{p_1(v) + p_3(v)}{S(v)} dv \\
        & = \frac{S(t)}{S(u)} \cdot V(u)
    \end{split}
\end{equation}
To verify the theoretical result, the process is simulated for $N = 100$ replications with 200 initial stem cells and division rate 0.2 and parameters $(p_1,p_2, c_1,c_2,m_1,m_2) = (0.5, 0.2, 0.005, 0.01, 4, 12)$ of the probability function. The stem cell counts are recorded at time points $t = \{5, 10, 15, 20, 25\}$. The empirical autocovariance are calculated
\begin{equation}
    \widehat{\mathbb{E}}[X(t_i)X(t_j)] -\widehat{\mathbb{E}}[X(t_i)] \widehat{\mathbb{E}}[X(t_j)]
    = \frac{1}{N} \sum_{r=1}^{N} X_r(t_i) \, X_r(t_j) - \frac{1}{N} \sum_{r=1}^{N} X_r(t_i) \, \frac{1}{N} \sum_{r=1}^{N} X_r(t_j).
\end{equation}
Normalized to autocorrelation, we have the theoretical autocorrelation 
\[
\text{Corr} (X(t_i),X(t_j)) =
\left[
\begin{array}{c|ccccc}
 & 5 & 10 & 15 & 20 & 25 \\ \hline
5 & 1.000 & 0.738 & 0.617 & 0.525 & 0.434 \\
10 & 0.738 & 1.000 & 0.837 & 0.711 & 0.588 \\
15 & 0.617 & 0.837 & 1.000 & 0.849 & 0.703 \\
20 & 0.525 & 0.711 & 0.849 & 1.000 & 0.827 \\
25 & 0.434 & 0.588 & 0.703 & 0.827 & 1.000 \\
\end{array}
\right],
\]
and the empirical autocorrelation
\[
\widehat{\text{Corr}} (X(t_i),X(t_j)) =
\left[
\begin{array}{c|ccccc}
 & 5 & 10 & 15 & 20 & 25 \\ \hline
5 & 1.000 & 0.714 & 0.500 & 0.366 & 0.286 \\
10 & 0.714 & 1.000 & 0.800 & 0.663 & 0.562 \\
15 &  0.500 & 0.800 & 1.000 & 0.841 & 0.726 \\
20 & 0.366 & 0.663 & 0.841 & 1.000 & 0.870  \\
25 & 0.286 & 0.562 & 0.726 & 0.870 & 1.000 \\
\end{array}
\right].
\]

To further quantify the similarity, I computed the mean absolute difference
\begin{equation}
    \text{MAD} = \frac{1}{T^2} \sum_{i,j} 
    \big| \widehat{\text{Corr}} (X(t_i),X(t_j)) - \text{Corr} (X(t_i),X(t_j)) \big|,
\end{equation}
and the Pearson correlation between the vectorized empirical and theoretical matrices. 
Both metrics indicate strong agreement (see Table~\ref{tab:autocorr_stats}).
\begin{table}[!ht]
\centering
\begin{tabular}{lc}
\toprule
Metric  & Value \\
\midrule
Mean Absolute Difference &  0.050 \\
Maximum Absolute Difference &  0.159 \\
Matrix Correlation &  0.982 \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison between theoretical and empirical autocorrelation matrices.}
\label{tab:autocorr_stats}
\end{table}
\newpage
\subsection*{Parameter estimates using differential evolution algorithm}
Since the estimation result using the gradient-based method is highly sensitive to the starting values, I'm trying different optimization method to find the MLE. The method I'm trying is differential evolution optimization algorithm, which is a method used to find minimum or maximum of a function for when the function is nonlinear, non-differentiable, or has many local optima. It works by iteratively searching for a better solution candiate in comparison to the previous one: 
\begin{itemize}
    \item DE creates a new trial solution candidate by adding the weighted difference between two randomly chosen individuals to a third one. 
    \item This new candidate is then mixed with the original solution candidate through crossover, and the better of the two is kept for the next generation. 
    \item Through repeated mutation, crossover, and selection, the population gradually moves toward the optimal solution.
\end{itemize}
Table (\ref{tab:de_est_case1}) and (\ref{tab:de_est_case2}) shows the estimate results for simulated data of two sets of parameters using the differential evolution algorithm with random starting values. The differential evolution algorithm is implemented from the package \textit{DEoptim} in R. The estimates are good in both cases. 

The only issue that might arise with implementing this algorithm is the constraints as we need to provide a finite bound for the parameters. For example, in our case, we have the constraint for parameters $c_1, c_2, m_1, m_2$ to be greater than 0. However, the algorithm requires the parameters to have finite bound, i.e $a_1 < c_1, c_2 <a_2, b_1 <m_1, m_2 < b_2$ for some constants $a_1, a_2, b_1, b_2$. When implementing this algorithm to obtain the result in table (\ref{tab:de_est_case1}) and (\ref{tab:de_est_case2}), I use the bounds $0 < c_1, c_2 < 100$ and $0 < m_1, m_2 < \max(\text{observed time)}$. I will try this optimization method with when we have $p_4 \neq 0$.
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ (0.55)& $p_2$ (0.15) &$c_1$ (0.005) &$c_2$ (0.01) & $m_1$ (4)& $m_2$ (18) \\
         \hline
         Mean & 0.558 & 0.153& 0.00515 & 0.011	& 3.780 & 18.100 \\
         Median	& 0.557 & 0.152 & 0.00507 & 0.010 &3.850& 18.200 \\
         2.5 Percentile&	0.504&	0.110&	0.00300& 0.00377 &	0.377 & 15.700 \\
         97.5 Percentile &	0.617&	0.193&	0.00762&	0.021&	6.450&	20.100\\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using differential evolution with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.55, 0.15, 0.005, 0.01, 4, 18)$.}
    \label{tab:de_est_case1}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         Parameter& $p_1$ (0.2) & $p_2$ (0.7) &$c_1$ (0.005) &$c_2$ (0.01) & $m_1$ (12) & $m_2$ (6) \\
         \hline
         Mean&	0.202&	0.701&	0.00543&	0.01030&	11.200&	6.030\\
         Median&	0.201&	0.699&	0.00490&	0.01020&	11.600&	6.080\\
         2.5 Percentile&	0.166&	0.640&	0.000973&	0.00675&	5.760&	4.730\\
         97.5 Percentile&	0.247&	0.766&	0.01150&	0.01500&	14.700&	7.090\\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using differential evolution with 100 replications. The true parameters are $(p_1, p_2, c_1 ,c_2, m_1,m_2) = (0.2, 0.7, 0.005,0.1, 12, 6)$.}
    \label{tab:de_est_case2}
\end{table}
\newpage

\subsection*{Add $p_4$ and use differential evolution for maximum likelihood estimates}

Define the time-dependent probability $p_4(\cdot)$ similar to $p_1(\cdot)$ and $p_2(\cdot)$

$$p_4(t) = \frac{p_4}{1+c_4(t-m_4)^2}.$$

Then the log-likelihood is
\begin{equation}
    \begin{split}
        & \ell(T_1, \cdots, T_n , S_0, \cdots, S_n, D_0, \cdots D_n) \\ 
        &= \sum_{i=1}^k \Big[ \log \Big(\frac{p_1}{1+c_1(T_i-m_1)^2}\Big) I_{(X_i = 1, Z_i = 0)} \\
        & + \log \Big(\frac{p_2}{1+c_2(T_i-m_2)^2}\Big) I_{(X_i = 0, Z_i = 0)} \\
        & + \log \Big(\frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(X_i = 0, Z_i = 1)} \\
        & + \log \Big(1- \frac{p_1}{1+c_1(T_i-m_1)^2}- \frac{p_2}{1+c_2(T_i-m_2)^2} - \frac{p_4}{1+c_4 (T_i-m_4)^2} \Big) I_{(X_i = -1, Z_i = 0)} \\
        & + \log r + \log S_{i-1} - rS_{i-1}\Delta_{T_1} \Big].
    \end{split}
\end{equation}
Since the MLE of $r$ has the closed form solution $\hat{r} = \frac{n}{\sum_{i=1}^n S_{i-1} \Delta_{T_i}}$, I applied the differential evolution optimization method to find the maximum likelihood estimates for the parameter of the probability functions $p_1, p_2, p_4, c_1, c_2, c_4, m_1, m_2, m_4$ by maximizing
\begin{equation*}
    \begin{split}
        A &= \sum_{i=1}^k \Big[ \log \Big(\frac{p_1}{1+c_1(T_i-m_1)^2}\Big) I_{(X_i = 1, Z_i = 0)} \\
        & + \log \Big(\frac{p_2}{1+c_2(T_i-m_2)^2}\Big) I_{(X_i = 0, Z_i = 0)} \\
        & + \log \Big(\frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(X_i = 0, Z_i = 1)} \\
        & + \log \Big(1- \frac{p_1}{1+c_1(T_i-m_1)^2}- \frac{p_2}{1+c_2(T_i-m_2)^2} - \frac{p_4}{1+c_4 (T_i-m_4)^2} \Big) I_{(X_i = -1, Z_i = 0)}].
    \end{split}
\end{equation*}
Table (\ref{tab:de_est_full01}) and (\ref{tab:de_est_full02}) show the estimate results for simulated data of two sets of parameters using differential evolution algorithm with random starting values. In both cases the estimates are good. However, compare to the when we only have parameters for $p_1(\cdot)$ and $p_2(\cdot)$ (i.e $p_4(t) = 0 \ \forall t$), the number of iterations required is much larger and thus results in longer computational time. It took between 30-40 minutes to obtain the each result in table (\ref{tab:de_est_full01}) and (\ref{tab:de_est_full02}).
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c}
    \hline
        Parameter & $p_1$ (0.55) & $p_2$ (0.20)  & $p_4$(0.15) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (4)  & $m_2$ (18)  & $m_4$ (28)  \\
        \hline
         Mean & 0.552 & 0.203 & 0.151 & 0.00528 & 0.0127 &  0.00876 & 4.070 & 17.900 & 28.200\\
         Median & 0.551 & 0.202 & 0.152 & 0.00497 & 0.0122 & 0.00842 & 4.120 & 17.900 & 28.000\\
         2.5 Percentile & 0.505 & 0.167 & 0.108 & 0.00333 & 0.00618 & 0.00302 & 0.914 & 16.200 & 25.100\\
         97.5 Percentile & 0.604 & 0.245 & 0.202 & 0.00834  & 0.0208 & 0.0156 & 6.450 & 19.700  & 32.300\\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using differential evolution with 100 replications. The true parameters are $(p_1, p_2, p_4, c_1 ,c_2, c_4, m_1,m_2, m4) = (0.55, 0.20, 0.15, 0.005, 0.012, 0.008, 4, 18, 28)$.}
    \label{tab:de_est_full01}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c}
    \hline
        Parameter & $p_1$ (0.20) & $p_2$ (0.70)  & $p_4$(0.10) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (12)  & $m_2$ (6)  & $m_4$ (28)  \\
        \hline
        Mean & 0.205 & 0.704 & 0.112 & 0.00594 & 0.0122 & 0.0122 & 11.800  & 5.960 & 27.600 \\
        Median & 0.206 & 0.705 & 0.102  & 0.00541 & 0.0125 & 0.00979  & 11.800 & 5.970 & 26.700\\
        2.5 Percentile & 0.163 & 0.649 & 0.049 & 0.00195 & 0.00831 & 0.00265  & 8.690 & 4.860 & 20.000\\
        97.5 Percentile & 0.250 & 0.773  & 0.208 & 0.0120 & 0.0165 & 0.0348 & 14.800 & 7.050  & 39.800 \\
         \hline
    \end{tabular}
    \caption{Parameters estimate results using differential evolution with 100 replications. The true parameters are $(p_1, p_2, p_4, c_1 ,c_2, c_4, m_1,m_2, m4) = (0.20, 0.70, 0.10, 0.005, 0.012, 0.008, 12, 6, 28)$.}
    \label{tab:de_est_full02}
\end{table}
\clearpage
\newpage
\subsection*{Estimation for observing data up until predetermined time $t^*$.}
Suppose we observe the events (both event time and division types) up until time $t^*$. 
The likelihood of observing the data up until this time point is
\begin{equation}
    \begin{split}
        &L_{t^*}(\boldsymbol{\theta}, \boldsymbol{T, X,Z}) \\
        & = \prod_{i=1}^n \Big[\frac{p_1}{1+c_1(T_i-m_1)^2} I_{(\Delta X_i = 1, \Delta Z_i = 0)} \\
        & + \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(\Delta X_i = 0, \Delta Z_i = 0)} \\
        & + \frac{p_4}{1+c_4(T_i-m_4)^2} I_{(\Delta X_i = 0, \Delta Z_i = 1)} \\
        & +  \Big(1 - \frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2} -\frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(\Delta X_i = -1, \Delta Z_i = 0)}\Big] \\
        & \cdot rX_{i-1} e^{-rX_{i-1} \Delta T_i} \cdot e^{-r X_n(t^*-T_n)}.
    \end{split}
\end{equation}
Then the log-likelihood
\begin{equation}
    \begin{split}
        & \ell_{t^*}(\boldsymbol{\theta}, \boldsymbol{T, X,Z}) \\
        & = \sum_{i=1}^n \Big[\log\frac{p_1}{1+c_1(T_i-m_1)^2} I_{(\Delta X_i = 1, \Delta Z_i = 0)} \\
        & + \log \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(\Delta X_i = 0, \Delta Z_i = 0)} \\
        & +\log \frac{p_4}{1+c_4(T_i-m_4)^2} I_{(\Delta X_i = 0, \Delta Z_i = 1)} \\
        & +  \log\Big(1 - \frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2} -\frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(\Delta X_i = -1, \Delta Z_i = 0)}\Big] \\
        & + \log r + \log X_{i-1} -rX_{i-1} \Delta T_i -r X_n(t^*-T_n).
    \end{split}
\end{equation}
Then the MLE of $\hat{r}$ has the closed-form
$$\hat{r} = \frac{n}{ \sum_{i=1}^n X_{i-1} \Delta T_i + X_n (t^* -T_n)}.$$
The estimates of $p_1, p_2, p_4, c_1, c_2,c_4,m_1,m_2,m_4$ are obtained by the maximimizing the quantity
\begin{equation*}
    \begin{split}
        A & = \sum_{i=1}^n \Big[\log\frac{p_1}{1+c_1(T_i-m_1)^2} I_{(\Delta X_i = 1, \Delta Z_i = 0)} \\
        & + \log \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(\Delta X_i = 0, \Delta Z_i = 0)} \\
        & +\log \frac{p_4}{1+c_4(T_i-m_4)^2} I_{(\Delta X_i = 0, \Delta Z_i = 1)} \\
        & +  \log\Big(1 - \frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2}\Big) I_{(\Delta X_i = -1, \Delta Z_i = 0)}\Big].
    \end{split}
\end{equation*}
Essentially, I'm doing the same optimization as before with less data (only using the data up until the time $t^*$).

Table (\ref{tab:tstop40}) through (\ref{tab:tstop10}) display the parameter estimate results of the same simulated data with different maximum observation times $t^* = 40, 30, 20, 10$. The estimates for $r$ are good in all cases. When $t^* = 40$ and $t^* = 30$ (table (\ref{tab:tstop40}) and (\ref{tab:tstop30}, respectively), the estimates are still good. However, the estimates for the probability parameters get worse as the maximum observation time gets smaller (when $t^* = 20$ and $t^* = 10$ in table (\ref{tab:tstop20}) and (\ref{tab:tstop10}), respectively). 

It's also interesting to note that when $t^* = 20$, the estimates for $p_1, p_2,c_1,c_2, m_1, m_2$ are still good and it coincides with the fact that the true values $m_1, m_2 < t^*$ while the true value of $m_4 > t^*$ in this case. I suspect that with $t^* = 20$, there were not enough events for $p_4(\cdot)$ observed to estimates the parameters of $p_4(\cdot)$. Similarly, when $t^* =10$, the estimates for $p_1(\cdot)$ is still pretty good but those for $p_2(\cdot)$ and $p_4(\cdot)$ are not, and when $t^* =10$, the true value $m_1 < t^*$ and the true values of $m_2, m_4 > t^*$.
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
        Parameter & $p_1$ (0.55) & $p_2$ (0.15)  & $p_4$(0.20) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (4)  & $m_2$ (18)  & $m_4$ (28)  \\
        \hline
         Mean & 0.556 & 0.156 & 0.207 & 0.00535 & 0.0138 &  0.00849 & 3.960 & 18.000 & 28.600\\
         Median & 0.557 & 0.155 & 0.204 & 0.00527 & 0.0139 & 0.00785 & 4.140 & 18.000 & 28.100\\
         2.5 Percentile & 0.500 & 0.123 & 0.162 & 0.00331 & 0.00657 & 0.00313 & 0.425 & 16.100 & 25.500\\
         97.5 Percentile & 0.609 & 0.190 & 0.263 & 0.00805  & 0.0231 & 0.0149 & 6.460 & 20.300  & 33.900\\
         \hline
    \end{tabular}
    \begin{tabular}{|c|c|}
    \hline
         Parameter & $r$ (0.2)  \\
         \hline
         Mean & 0.199\\
         Median & 0.199\\
         2.5 Percentile & 0.189 \\
         97.5 Percentile & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimates using data up until time point $t^* = 40$. The estimates of the probability parameters $p_i,c_i,m_i$ are obtained by the optimize the quantity $A$ with differential evolution and the estimates of $r$ are obtained by the closed-form solution. The true parameters are $(p_1,p_2,p_4, c_1,c_2,c_4,$$m_1,m_2,m_4,r) =$ $(0.55, 0.15, 0.20, 0.005, 0.012, 0.008,$$4, 18, 28, 0.2)$.}
    \label{tab:tstop40}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
        Parameter & $p_1$ (0.55) & $p_2$ (0.15)  & $p_4$(0.20) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (4)  & $m_2$ (18)  & $m_4$ (28)  \\
        \hline
         Mean & 0.556 & 0.158 & 0.265 & 0.00536 & 0.0142 &  0.00959 & 3.962 & 17.900 & 29.900\\
         Median & 0.560 & 0.156 & 0.218 & 0.00534 & 0.0139 & 0.00899 & 4.340 & 17.800 & 27.300\\
         2.5 Percentile & 0.497 & 0.125 & 0.149 & 0.00332 & 0.00610 & 0.00381 & 0.296 & 15.800 & 24.200\\
         97.5 Percentile & 0.606 & 0.192 & 0.840 & 0.00840  & 0.0259 & 0.0171 & 6.470 & 20.400  & 47.800\\
         \hline
    \end{tabular}
    \begin{tabular}{|c|c|}
    \hline
         Parameter & $r$ (0.2)  \\
         \hline
         Mean & 0.199\\
         Median & 0.199\\
         2.5 Percentile & 0.188 \\
         97.5 Percentile & 0.209\\
         \hline
    \end{tabular}
    \caption{Parameter estimates using data up until time point $t^* = 30$. The estimates of the probability parameters $p_i,c_i,m_i$ are obtained by the optimize the quantity $A$ with differential evolution and the estimates of $r$ are obtained by the closed-form solution. The true parameters are $(p_1,p_2,p_4, c_1,c_2,c_4,$$m_1,m_2,m_4,r) =$ $(0.55, 0.15, 0.20, 0.005, 0.012, 0.008,$$4, 18, 28, 0.2)$.}
    \label{tab:tstop30}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
        Parameter & $p_1$ (0.55) & $p_2$ (0.15)  & $p_4$(0.20) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (4)  & $m_2$ (18)  & $m_4$ (28)  \\
        \hline
         Mean & 0.559 & 0.236 & 0.575 & 0.00579 & 0.0189 &  0.0259 & 4.120 & 19.400 & 28.700\\
         Median & 0.560 & 0.167 & 0.670 & 0.00563 & 0.0174 & 0.0258 & 4.230 & 17.300 & 29.900\\
         2.5 Percentile & 0.500 & 0.119 & 0.0736 & 0.00292 & 0.00527 & 0.00367 & 0.827 & 14.200 & 15.600\\
         97.5 Percentile & 0.610 & 0.908 & 0.990 & 0.00928  & 0.0416 & 0.0593 & 6.600 & 33.000  & 40.900\\
         \hline
    \end{tabular}
    \begin{tabular}{|c|c|}
    \hline
         Parameter & $r$ (0.2)  \\
         \hline
         Mean & 0.199\\
         Median & 0.199\\
         2.5 Percentile & 0.188 \\
         97.5 Percentile & 0.211\\
         \hline
    \end{tabular}
    \caption{Parameter estimates using data up until time point $t^* = 20$. The estimates of the probability parameters $p_i,c_i,m_i$ are obtained by the optimize the quantity $A$ with differential evolution and the estimates of $r$ are obtained by the closed-form solution. The true parameters are $(p_1,p_2,p_4, c_1,c_2,c_4,$$m_1,m_2,m_4,r) =$ $(0.55, 0.15, 0.20, 0.005, 0.012, 0.008,$$4, 18, 28, 0.2)$.}
    \label{tab:tstop20}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
        Parameter & $p_1$ (0.55) & $p_2$ (0.15)  & $p_4$(0.20) & $c_1$ (0.005) & $c_2$ (0.012)  & $c_4$ (0.008)  & $m_1$ (4)  & $m_2$ (18)  & $m_4$ (28)  \\
        \hline
         Mean & 0.572 & 0.490 & 0.450 & 0.00815 & 0.125 &  0.139 & 4.030 & 14.900 & 13.800\\
         Median & 0.572 & 0.537 & 0.465 & 0.00701 & 0.0914 & 0.0837 & 4.140 & 15.300 & 14.800\\
         2.5 Percentile & 0.502 & 0.0629 & 0.0470 & 0.00077 & 0.00544 & 0.00769 & 0.386 & 4.260 & 0.898\\
         97.5 Percentile & 0.644 & 0.983 & 0.975 & 0.0197  & 0.375 & 0.577 & 8.440 & 26.700  & 25.800\\
         \hline
    \end{tabular}
    \begin{tabular}{|c|c|}
    \hline
         Parameter & $r$ (0.2)  \\
         \hline
         Mean & 0.200\\
         Median & 0.201\\
         2.5 Percentile & 0.186 \\
         97.5 Percentile & 0.214\\
         \hline
    \end{tabular}
    \caption{Parameter estimates using data up until time point $t^* = 10$. The estimates of the probability parameters $p_i,c_i,m_i$ are obtained by the optimize the quantity $A$ with differential evolution and the estimates of $r$ are obtained by the closed-form solution. The true parameters are $(p_1,p_2,p_4, c_1,c_2,c_4,$$m_1,m_2,m_4,r) =$ $(0.55, 0.15, 0.20, 0.005, 0.012, 0.008,$$4, 18, 28, 0.2)$.}
    \label{tab:tstop10}
\end{table}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{probability_plot_055015020.png}
    \caption{Functions $p_1(t),p_2(t), p_3(t), p_4(t)$ with parameters $(p_1,p_2,p_4, c_1,c_2,c_4,$$m_1,m_2,m_4,r) =$ $(0.55, 0.15, 0.20, 0.005, 0.012, 0.008,$$4, 18, 28, 0.2)$}
    \label{fig:placeholder}
\end{figure}
\clearpage
\subsection*{Forward algorithm for when viable and non-viable stem cells are not observed}
Let $(X_{t_i}, Y_{t_i} Z_{t_i})$ denote numbers of viable stem cells, differentiated cells, and nonviable stem cells. Assume that we can not differentiate between viable and non-viable stem cells, we have the hidden states $$ V_{t_i} = (X_{t_i}, Y_{t_i}, Z_{t_i}) $$we can only observe $$M_{t_i} = X_{t_i} + Z_{t_i} \ \text{and} \ Y_{t_i}.$$
Then, the probability of observing $(M_{t_i}, Y_{t_i})$ given the hidden states $V_{t_i} = (X_{t_i}, Y_{t_i}, Z_{t_i})$ (emission probability) is $$P( (M_{t_i}, Y_{t_i}) \vert V_{t_i}) = I_{(M_{t_i} =X_{t_i} + Z_{t_i})}.$$
Each hidden state $v = (x,y,z)$ can transition to a new states $v' = (x', y', z')$ through one of the four events. 
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        Event $k$ & Probability & $\Delta X$  & $\Delta Y$  & $\Delta Z$ \\
        \hline
        1 & $p_1(t_i)$ & +1 & 0  & 0\\
        2 & $p_2(t_i)$ & 0  & +1 & 0\\
        3 & $1- p_1(t_i)- p_2(t_i) - p_4(t_i)$ & -1 & 2  & 0\\
        4 & $p_4(t_i)$ & 0  & 0  & +1\\
        \hline
    \end{tabular}
    \caption{Division event type and resulting change of cell counts.}
    \end{table}
The transition probability of transitioning from hidden state $v = (x,y,z)$ to the new hidden state $v' = (x',y',z')$ is
$$P(V_{t_i} = v' \vert V_{t_{i-1}} = v) = rx e^ {-rx \Delta t_i}\sum_{k=1}^4 p_k (t_i) I_{(v' = v + \Delta V_k)}.$$
We want the likelihood of observed data sequence of cell counts and of even time $$P((M_{T_1},Y_{T_1}), (M_{T_2}, Y_{T_2}), \cdots, (M_{T_n}, Y_{T_n}), T_1, \cdots T_n).$$
We define the forward variable $\alpha_i(v)$ as the probability of being in hidden state $v = (x,y,z)$ after $i$-th event, and having seen all previous observation. Then, the forward recursion is
\begin{equation}
    \begin{split}
        \alpha_i(v') = \sum_{v\in \mathcal{V}} \alpha_{i-1}(v) P(V_{t_i} = v' \vert V_{t_{i-1}} = v) P( (M_{t_i}, Y_{t_i}) \vert V_{t_i}).
    \end{split}
\end{equation}
\textbf{Number of hidden states exploding}

I'm trying to figure out how to address the issue of number of hidden states exploding as the number of division increases. For example, 
\begin{itemize}
    \item after the first division, there are three possible hidden states with number of stem cells as $X_{t_1} = \{S_0-1, S_0, S_0 + 1\}$
    \item after the second division, there are five possible hidden states with number of stem cells $X_{t_2} = \{S_0 - 2, S_0 -1, S_0, S_0 +1, S_0 + 2\}.$
\end{itemize}
However, since we can observe the number of differentiated cells $Y_{t_i}$ and how it changes, we know some of these hidden states are not possible. For example, after the first division
\begin{itemize}
    \item if $\Delta Y_{t_1} = 1$, then $X_{t_1} = S_0$,
    \item if $\Delta Y_{t_1} = 2$, then $X_{t_1} = S_0+2$,
    \item $\Delta Y_{t_1} = 0$, then $X_{t_1} = \{S_0, S_1\}$.
\end{itemize}
Maybe this could be used to eliminate some of the possible hidden states.

Another consideration is that the hidden states directly affect the interarrival times since the rate of arrival times depends on the number of viable stem cells, but the probability of event types given that we observe the interarrival times does not. In other word, given the event times $T_i$'s, the conditional likelihood of observing the cell count change is 
\begin{equation*}
    \begin{split}
        L(M_{t_i}, Y_{t_i} \vert T_i) & = \prod_{i=1}^n \Big[ \Big(\frac{p_1}{1+c_1(T_i-m_1)^2} + \frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(\Delta M_i = 1, \Delta Y_i = 0)} \\
        & + \frac{p_2}{1+c_2(T_i-m_2)^2} I_{(\Delta M_i = 0, \Delta Y_i = 1)} \\
        & +  \Big(1 - \frac{p_1}{1+c_1(T_i-m_1)^2} - \frac{p_2}{1+c_2(T_i-m_2)^2} -\frac{p_4}{1+c_4(T_i-m_4)^2}\Big) I_{(\Delta M_i = -1, \Delta Y_i = 2)}\Big].
    \end{split}
\end{equation*}
I'm not quite sure if this could be useful to simplify the expression/calculation of the forward variable yet.
\clearpage
\subsection*{Forward algorithm}
Let $(X_{i}, Y_{i}, Z_{i})$ denote viable stem cells, differentiated cells, and non-viable stem cells and $T_i$ are event times. Let $M_{i}$ denote the sum of the viable and non-viable stem cells. Note that $(M_{i},Y_{i}, T_{i}), \ k =1,2,\cdots, n$ are observable and 
$$T_0 = 0, M_0 = S_0, Y_0 = 0, Z_0 = 0.$$
Forward variables (for $k = 1, \cdots, n$)
$$\alpha_k(u) = \ell(T_1, \cdots, T_k, Y_1, \cdots Y_k, M_1, \cdots M_k, Z_k = u),$$
where $u$ is an integer, $0 \leq z \leq M_k$. Then for $0 \leq k \leq n-1$,
$$\alpha_{k+1} (v) = \sum_{u \in\{v, v-1\}} \alpha_k(u) \cdot h_k(u,v),$$
where $0 \leq v \leq M_{k+1}$. Here $h_k(u,v)$ are transitional "probabilities, and for $i \leq m \leq 4$
\begin{equation*}
    \begin{split}
        h_k(u,v) = \begin{cases}
            h_{mk}(u,v), & \text{event}\ m \ \text{at} \ T_{k+1}\\
            0, & \text{otherwise},
        \end{cases}
    \end{split}
\end{equation*}
$h_{mk}(u,v) = r(M_k-u) e^{-r(M_k-u) (T_{k+1}-T_k)}\cdot p_m(T_{k+1})$.

The events in terms of $(Y,M,Z)$
\begin{itemize}
    \item Event 1: $Y_{k+1} = Y_k, M_{k+1} = M_k +1, v = u$
    \item Event 2: $Y_{k+1} = Y_k +1, M_{k+1} =M_k, v = u$
    \item Event 3: $Y_{k+1} = Y_k +2, M_{k+1} = M_k -1, v = u$
    \item Event 4: $Y_{k+1} = Y_k, M_{k+1} = M_k + 1, v = u+1$
\end{itemize}
Finally, the likelihood is given by $$L_n = \sum_{0 \leq u \leq M_n} \alpha_n(u).$$
\subsection*{Normalization}
Let $$L_k = \sum_{0 \leq u \leq M_k} \alpha_k (u), \quad k=1, \cdots, n.$$
Consider $\bar{\alpha}_k(v) = \frac{\alpha_k(v)}{L_k}$ then
\begin{equation}
    \begin{split}
        \bar{\alpha}_{k+1}(v) & = \frac{\alpha_{k+1} (v)}{L_{k+1}} \\
        & = \frac{1}{L_{k+1}} \sum_{u = v-1,v} \alpha_k(u) h_k(u,v) \\
        & = \frac{L_k}{L_{k+1}} \sum_{u = v-1,v} \bar{\alpha}_k h_k(u,v).
    \end{split}
\end{equation}
Now for $0 \leq k \leq n-1$ let
\begin{equation}
    \begin{split}
        d_{k+1} & = \frac{L_{k+1}}{L_k} \\
        & = \frac{1}{L_k} \sum_{0 \leq v\leq M_{k+1}} \alpha_{k+1}(v) \\
        & = \frac{1}{L_k} \sum_{0 \leq v \leq M_{k+1}} \sum_{u = v-1, v} \alpha_k(u) h_k (u,v)\\
        & = \sum_{0\leq v\leq M_{k+1}} \sum_{u=v-1,v} \bar{\alpha_k(u)} h_k(u,v)
    \end{split}
\end{equation}
So, $\bar{\alpha}_0(0) = 1$ and $\bar{\alpha}_k \rightarrow d_{k+1} \rightarrow \bar{\alpha}_{k+1}.$
$$\log(L_n)=\sum_{k=0}^{n-1}  \log(d_{k+1})$$

\subsection*{Code implementation}

\begin{algorithm}
\caption{Forward Log-Likelihood Computation}
\begin{algorithmic}[1]

\STATE Extract $M, Y, t$ from data
\STATE Extract parameters $p_1, p_2, p_4, c_1, c_2, c_4, m_1, m_2, m_4, r$

\STATE Initialize hidden states: $H \gets \{0\}$
\STATE $\alpha_{\text{prev}} \gets \mathbf{1}$ (vector of ones)
\STATE $n \gets \text{length}(t)$
\STATE $\text{scale}[1] \gets 1$

\FOR{$k = 2$ to $n$}

    \STATE $\Delta T = t_k - t_{k-1}$
    \STATE $\Delta Y = Y_k - Y_{k-1}$
    \STATE $\Delta M = M_k - M_{k-1}$

    \STATE Compute timeâ€“varying probabilities:
    \[
    p_{1,t} = \frac{p_1}{1 + c_1 (t_k - m_1)^2}, \quad
    p_{2,t} = \frac{p_2}{1 + c_2 (t_k - m_2)^2}, \quad
    p_{4,t} = \frac{p_4}{1 + c_4 (t_k - m_4)^2}
    \]

    \IF{$\Delta Y = 1$ and $\Delta M = 0$}
        \STATE Construct diagonal transition matrix $T$
        \FOR{each state $h \in H$}
            \STATE $\lambda = r (M_{k-1} - h)$
            \STATE $T_{hh} = f_{\exp}(\Delta T; \lambda) \cdot p_{2,t}$
        \ENDFOR
        \STATE $\alpha_{\text{new}} = \alpha_{\text{prev}} T$

    \ELSIF{$\Delta Y = 2$ and $\Delta M = -1$}
        \STATE $p_{3,t} = \max(1 - p_{1,t} - p_{2,t} - p_{4,t}, \varepsilon)$
        \STATE Construct diagonal transition matrix $T$
        \FOR{each state $h \in H$}
            \STATE $\lambda = r (M_{k-1} - h)$
            \STATE $T_{hh} = f_{\exp}(\Delta T; \lambda)\cdot p_{3,t}$
        \ENDFOR
        \STATE $\alpha_{\text{new}} = \alpha_{\text{prev}} T$
        \STATE Remove states in $H$ exceeding $M_k$
        \STATE Truncate $\alpha_{\text{new}}$ accordingly

    \ELSIF{$\Delta Y = 0$ and $\Delta M = 1$}
        \STATE Construct transition matrix $T$ with one extra column
        \FOR{each state $h \in H$}
            \STATE $\lambda = r(M_{k-1} - h)$
            \STATE $T_{h,h}     = f_{\exp}(\Delta T; \lambda) \cdot p_{1,t}$
            \STATE $T_{h,h+1}   = f_{\exp}(\Delta T; \lambda) \cdot p_{4,t}$
        \ENDFOR
        \STATE $\alpha_{\text{new}} = \alpha_{\text{prev}} T$
        \STATE Add new state to $H$ if $\le M_k$
        \STATE Truncate $\alpha_{\text{new}}$ to match $H$
    \ENDIF

    \STATE $\alpha_{\Sigma} = \sum_i \alpha_{\text{new}}[i]$
    \IF{$\alpha_{\Sigma} \le \varepsilon$}
        \STATE $\alpha_{\Sigma} \gets \varepsilon$
    \ENDIF

    \STATE $\text{scale}[k] = \alpha_{\Sigma} / \text{scale}[k-1]$
    \STATE $\alpha_{\text{prev}} = \alpha_{\text{new}} / \alpha_{\Sigma}$

\ENDFOR

\STATE Compute log-likelihood:
\[
\ell = \sum_{k=1}^n \log(\text{scale}[k])
\]
\RETURN $-\ell$

\end{algorithmic}
\end{algorithm}

\end{document}

